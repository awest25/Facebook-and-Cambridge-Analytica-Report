# Facebook’s Role in the Cambridge Analytica Scandal

<img width="621" alt="image" src="https://github.com/awest25/Facebook-and-Cambridge-Analytica-Report/assets/93575706/12d2f4ea-0151-493e-8aa8-b7edf0cfbc68">

**Alexander West** | March 8th, 2024

## Table of Contents  

- [Abstract](#abstract)
- [How it all unfolded](#how-it-all-unfolded)
- [Facebook’s Engineering Failures](#facebooks-engineering-failures)
- [Ethical Analysis](#ethical-analysis)
- [Recommendations](#recommendations)
- [Conclusion](#conclusion)
- [Works Cited](#works-cited)

## Abstract

No information was hacked and no information was stolen, yet Cambridge Analytica was able to extrapolate voter profiles for over 50 million Americans based on their Facebook activity. Facebook management permitted multiple engineering oversights, including the failure to establish sufficient policies for user privacy protection through their Graph API and the insufficient enforcement of existing policies. In 2019, these failures led to a record-breaking $5 billion dollar fine. The actions Facebook management took are unethical when observed through the lens of deontology since they cannot be universalized to other technology companies, and their actions violate the moral autonomy of its users failing the reciprocity principle. Moving forward, the management of companies in the tech industry should proactively test their APIs for vulnerabilities and enforce their guidelines to ensure third-party developers follow them.

## How it all unfolded

Facebook is known for its questionable handling of user data. Beginning in 2010, third-party apps were given extensive access to the data of Facebook users. This data included not only the data of the users of the app but also that of their friends, who had never interacted with the app directly (CPO Team 2018). This was known to developers as the Open Graph Application Programming Interface (API). Facebook engineers as well as executives were aware of the potential for misuse from this but did not take action. This is because they valued unique integrations from third-party apps over the privacy risk posed. If integrations were given access to more data, developers would have wider freedoms to create unique apps to indirectly keep people using Facebook.

Privacy advocates and researchers contacted Facebook regarding the potential for data misuse. Facebook themselves reached out to a separate group of developers misusing the API with the “the threat of a lawsuit and a job offer” (Romano 2018). In addition, public pressure was mounting to overhaul nonconsensual data collection and sharing during this time. This culminated in Facebook shutting down the API in 2015 and asking developers to delete the data that they collected during its operation.

One of the developers utilizing the API was Russian American Aleksandr Kogan, an academic who worked at the University of Cambridge. He had created an app named thisisyourdigitallife which was marketed at a select group of Facebook users which were more than willing to give their data to a third-party app. In the fine print was written that the app was collecting data for the purposes of academic research, but most users saw it as an opportunity to take a personality test (Romano 2018).

Kogan’s app soon gained 270,000 users which offered up their information. But thanks to the Open Graph API, they also offered the information of their total of 50 million friends. Kogan then saw the profit opportunity in this and began marketing his newly acquired dataset. Soon enough, he found an interested party: Strategic Communication Laboratories (SLC) Group. According to Vox’s Andrew Prokop:

> SCL tends to describe its capabilities in grandiose and somewhat unsettling language — the company has touted its expertise at ”psychological warfare” and “influence operations.” It’s long claimed that its sophisticated understanding of human psychology helps it target and persuade people of its clients’ preferred message. (Chang 2018)

The data collected by Kogan found a home when conservative mega donor Robert Mercer was approached by Steve Bannon in hopes of starting a political consulting firm. To accomplish this, they created an American shell company named Cambridge Analytica (Chang 2018).

Facebook caught wind of the way this company was “harvesting data at an unprecedented scale” which was the reason for the API change in 2015. Following their discovery, Facebook reached out to Cambridge Analytica and “demanded that CA certify that it had destroyed all remnants of the data set” (Romano 2018). An employee at the political consulting sent Facebook a signature indicating the data had been deleted. This employee later turned out to be a whistleblower and detailed the fact that no action had been taken to remove the data (Ng 2018).

At this point, the information of 50 million people, including a vast array of traits that the company would later leverage to extract political preferences, was in the hands of a company with a goal to influence the next United States election. In its efforts to win the 2016 election, the Ted Cruz campaign approached Cambridge Analytica to receive information on voters’ psychological data on voters in the year before. Then in 2016, Donald Trump’s campaign was looking for effective Facebook video advertising promoting him, and they began utilizing Cambridge Analytica’s services. Trump ended up being victorious in the election, making him the 45th president of the United States.

In the fallout of the whistleblowing and subsequent discovery of the companies’ actions, the Federal Trade Commission (FTC) fined Facebook $5 billion in July 2019, which was the largest fine ever imposed on a company for violating consumers' privacy. Cambridge Analytica ceased operations and filed for insolvency in May 2018, citing a loss of clients and mounting legal fees as the scandal unfolded. The scandal highlighted the importance of enacting sweeping personal data protection, and this made the European Union implement the General Data Protection Regulation (GDPR) in May of 2018.

## Facebook’s Engineering Failures

Many poor engineering decisions were made by Facebook leading up to the inappropriate use of user information by Cambridge Analytica. The issue that opened the doors for misuse was management’s instruction to expand the Open Graph API. In 2007, when Facebook first launched APIs to allow third-party developers to create applications that interact with core Facebook features, developers began utilizing the opportunity to build off of an established software. At this time, no significant privacy exploits took place. As time went on, Facebook added features to its APIs that not only helped developers expand the reach of their apps but also exposed more user information.

In 2010, Facebook engineers, guided by management decisions, introduced their Open Graph protocol, allowing even deeper integration for third-party websites and apps by allowing them to become part of Facebook's social graph, their representation of the network of connections among individuals. With the Open Graph API, third-party apps and websites could insert themselves into this social graph. For example, when a user interacts with a website (such as liking a news article or checking into a location), these actions could be shared on their Facebook timeline and news feed, effectively weaving the third-party service into the network of social interactions on Facebook. Management succeeded in their goal to expand their vision and the capabilities of the available APIs up until 2014, with each iteration adding data that third-parties had access to, including the data of friend’s profiles. They did this without enough regard for the potential misuse of the data (Burkhardt 2020).

A second failure following the problematic API was its oversight of the terms of service. Facebook’s management did not have an adequate policy in place for monitoring and enforcement of the privacy practices of third-party apps, as seen by the way thisisyourdigitallife was able to exploit Facebook’s features. In the case of Kogan’s application, data was gathered misleadingly under the guise of academic research before being used for political consulting purposes. According to its End User Agreement,

> We [Global Research Studies, a research organisation registered in England] use this … as part of our research on understanding how people's Facebook data can predict different aspects of their lives. Your contribution and data will help us better understand relationships between human psychology and online behaviour. (thisisyourdigitallife).

While the research is innocent, selling the data to use it for consulting was a deliberate strategy to bypass consent. This use was against Facebook’s terms of service, which prohibited the “transfer of any data … [for] monetization-related service.” But, like most terms of service agreements, they are a means to punish those who violate their contents, not a way to prevent misuse. This means that any sort of consequences would be delayed and by the time Facebook took action, significant amounts of data would already be misused (Wagner 2018).

Additionally to proactive oversight, Facebook’s management had inadequate policies dealing with responses to violations and thus deterrents from misuse. Though, as discussed with the signature certification of deletion, thisisyourdigitallife was a prime example of this, it is far from the only one. In fact, before Cambridge Analytica utilized data from it, they approached an early predecessor called MyPersonality that paved the way for the data collection app that they would eventually use. MyPersonality included very similar features as it was a personality quiz made by a research group for the purpose of finding out how much data it is possible to collect about people based on their social media account (Romano 2018). Its owners acted ethically after being approached by SLC and refused to engage with the group in fear that data would be misused. Still, data collection was extremely invasive and Facebook’s only response was “the threat of a lawsuit and a job offer” referenced previously (Romano 2018). This kind of response sends the message to developers that there is no punishment for data misuse and advances Facebook's interests instead of those of their consumers.

A third failure responsible for this data misuse was the regulatory environment related to data collection at the time. As far as proactive data protection, frameworks for digital data protection, especially concerning social media platforms and their third-parties, were not robust enough. Because of the rapidly-evolving nature of the industry, laws had not kept pace with the data harvesting practices of the time. The same goes for informed consent laws. Users were often unaware that their data was being accessed not just by apps they interacted with directly but also by other entities those apps shared data with. There was a significant dependence on self-regulation by companies like Facebook, allowing considerable leeway in how user data was handled, often prioritizing business interests over privacy concerns. To add to this, enforcing a consistent standard was challenging as Facebook was a global entity which spanned multiple jurisdictions that had varying levels of data protection laws (Ghorashi).

An example of this weak regulation was the Federal Trade Commission (FTC) consent decree with Facebook established in 2011. Provisions of this included requiring “affirmative express consent” before changing users’ privacy settings, establishing a privacy program, and being audited to ensure compliance with the program. The misuse of data shows that the oversight mechanisms were not enough and how they depended on the information that Facebook provided to auditors. And the decree did not address third-party data usage, underscoring the need to broaden the scope of legislation. For the final nail in the coffin, when the FTC investigated whether Facebook had violated the 2011 decree in the wake of the Cambridge Analytica scandal, the company settled (Hu).

## Ethical Analysis

It’s important to evaluate the situation with a lens of ethical analysis. Deontology is a branch of normative ethics that places emphasis on following duties or moral rules, regardless of consequences. Its main principles are the universality principle, stating that the moral principles must be applicable to all people, and reciprocity, which ensures that people cannot act selfishly by ensuring everybody’s free will remains. Finally, deontology emphasizes that people’s moral autonomy, or agency, must remain intact, so they can make their own decisions.

When implementing the Open Graph API, management at Facebook instructed engineers to follow the maxim that “companies should try to expand the reach of their products.” This is not a maxim that can be universalized. When applied to social media companies, the data of consumers has the potential of being misused which would lead them to lose trust in the company. This loss of trust means that the users of their products will move toward competitors who prioritize privacy, meaning that their products will shrink in user counts. Because this contradicts the maxim, the action of allowing third-party developers access to the data of friends was unethical. Additionally, the reciprocity principle is violated by opening up unconsenting users’ data to developers. As friends’ data was passively available, the friends were not aware of the privacy implications of using the product. This means that they lost their right to make an informed decision about whether to use Facebook, which makes the usage of the API unethical since it violates the reciprocity principle.

By not placing enough resources into ensuring that the policies for their software were followed, Facebook’s management were following the maxim of “companies should increase the integration of their platform into other products by trusting third-party developers to develop apps as they see fit.” When universalized to all platforms that support third-party API integrations, this maxim will allow developers to take advantage of the leniency provided to them to intrude too far into user data. Once it comes to light that these integrations are at the cost of privacy, regulators will step in to curb the amount of detail contained in user data, which would lower the level of integration supported by the platform, contradicting the maxim and thus making the action unethical. When considering the reciprocity principle, Facebook management’s lack of enforcement of its policies regarding third-party developers leads to misuse of user data going unnoticed and therefore unaddressed. When users engage with Facebook’s product, they expect a higher level of data privacy than offered by the policy in place, meaning they aren’t fully aware of the consequence of their decision and therefore have their moral autonomy hindered, making actions by Facebook management unethical.

## Recommendations

In hindsight, Facebook’s management should have investigated the potential consequences of opening an API with such a broad reach and implemented additional policies to enforce privacy standards. But moving forward, the managers at technology companies should prevent such issues by 

First and foremost, steps should be taken to research the implications of APIs that a company deploys. This involves hiring red teammers to attempt to misuse the API in a way that somebody with malicious intent would. It has been shown that red teaming is an advanced and effective way to identify weaknesses and address them prior to the launch of software (Wood 2000). To assess the findingings, an ethical and legal strategy committee should weigh the benefits of the API against its potential for misuse. This step needs to be throughout or after the development process and, importantly, before the release of the API to the public. By implementing this strategy, the potential legal and ethical consequences will be fully understood before people put their trust into the product.

Looking at this from a lens of deontology, when managers implemented this strategy they followed the maxim of “prevent misuse of user data to encourage people to use the product.” When universalized to all technology companies, users of their products will see the company’s policies and gain trust in the company, making them more likely to use the product. Since this aligns with the maxim, it passes the universality principle. As the company thoroughly researches how user’s data will be used, they can describe this in their user-facing policy. This means that the people using their products will be able to make an informed decision to use the product which satisfies the reciprocity principle. Because both principles are consistent, this action is the ethical way for companies to research their API.

In terms of policy enforcement, managers at technology companies must allocate resources to enforcing privacy policies and punishing parties that violate them. These resources should include a dedicated Privacy Compliance and Enforcement Team. This team's responsibilities would include monitoring third-party API usage for compliance with privacy policies, conducting regular audits, and implementing punitive measures against parties that violate these policies. Audits and monitoring should take place during third-party development as well as perpetually after the release of the product. Punitive damages should be carefully crafted to deter misuse but allow for third parties to respond to violations with revisions. This is to ensure there is a controlled environment about the product without making the liability of using the API too great to integrate with it. These actions should be a constantly evolving process, responding to the regulatory environment of the time as well as the public’s outlook on privacy. 

When viewed through the lens of deontology, this action can be said to follow the maxim of “protect the privacy of those who use your product.” Universalizing this to companies in the technology industry means that third-party developers who try to misuse integrations will be stopped and punished, deterring other products from violating the privacy policy. When companies must follow the company’s policies, user privacy is protected which is the goal of the maxim. As before, users will maintain their moral autonomy to choose whether to engage with the product since proper enforcement ensures the product uses their data as described.

## Conclusion

Studying Facebook’s failures shows how user privacy can be easily compromised when vulnerabilities are not prevented and enforcement actions are not taken. The Facebook and Cambridge Analytica scandal shows that even the largest companies are not immune to engineering failures, as well as revealing the lack of regulation surrounding the treatment of user data by social media products. Fortunately, the lessons learned from Facebook’s pitfalls can be applied to other technology companies’ products and APIs.

Some action has already been taken to mitigate future vulnerabilities of this kind. In the European Union, General Data Protection Regulation (GDPR) has proven to be successful in reshaping the data privacy landscape and in the United States, California is spearheading similar successful regulation with California Consumer Privacy Act (CCPA). As technology is a fast-evolving field, there will always be work to be done and management needs to be aware of the steps to take to safeguard user privacy.

## Works Cited

Burkhardt, Marcus. “THE EVOLUTION OF FACEBOOK'S GRAPH API.” AoIR Selected Papers of Internet Research, 5 October 2020, https://spir.aoir.org/ojs/index.php/spir/article/view/11185. Accessed 21 February 2024.
Chang, Alvin. “The Facebook and Cambridge Analytica scandal, explained with a simple diagram.” Vox, 2 May 2018, https://www.vox.com/policy-and-politics/2018/3/23/17151916/facebook-cambridge-analytica-trump-diagram. Accessed 20 February 2024.
Confessore, Nicholas. “Cambridge Analytica and Facebook: The Scandal and the Fallout So Far (Published 2018).” The New York Times, 4 April 2018, https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html. Accessed 20 February 2024.
CPO Team. “Inside the Facebook Cambridge Analytica Data Scandal.” CPO Magazine, 22 April 2018, https://www.cpomagazine.com/data-privacy/inside-the-facebook-cambridge-analytica-data-scandal/. Accessed 20 February 2024.
Ghorashi, Seyed Ramin. “An Analytical Review of Industrial Privacy Frameworks and Regulations for Organisational Data Sharing.” MDPI, 2023, https://www.mdpi.com/2076-3417/13/23/12727. Accessed 21 February 2024.
Hu, Margaret. “Cambridge Analytica’s black box.” Sage Journals, 2020, https://journals.sagepub.com/doi/10.1177/2053951720938091#bibr28-2053951720938091. Accessed 21 February 2024.
Ng, Alfred. “Facebook's 'proof' Cambridge Analytica deleted that data? A signature.” CNET, 16 May 2018, https://www.cnet.com/news/politics/facebook-proof-cambridge-analytica-deleted-that-data-was-a-signature/. Accessed 21 February 2024.
Romano, Aja. “The Facebook data breach wasn't a hack. It was a wake-up call.” Vox, 20 March 2018, https://www.vox.com/2018/3/20/17138756/facebook-data-breach-cambridge-analytica-explained. Accessed 20 February 2024.
thisisyourdigitallife. THISISYOURDIGITALLIFE APP APPLICATION END USER TERMS AND CONDITIONS, https://www.blumenthal.senate.gov/imo/media/doc/Facebook%20App%20Terms%20of%20Service.pdf. Accessed 21 February 2024.
van der Vlist, Fernando. “API Governance: The Case of Facebook’s Evolution.” Sage Journals, 7 May 2022, https://journals.sagepub.com/doi/10.1177/20563051221086228. Accessed 21 February 2024.
Wagner, Kurt. “Here's how Facebook allowed Cambridge Analytica to get data for 50 million users.” Vox, 17 March 2018, https://www.vox.com/2018/3/17/17134072/facebook-cambridge-analytica-trump-explained-user-data. Accessed 21 February 2024.
Wood, B.J. “Red Teaming of advanced information assurance concepts.” IEEE, 2000, https://ieeexplore.ieee.org/document/821513.
